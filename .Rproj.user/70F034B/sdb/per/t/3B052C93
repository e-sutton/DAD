{
    "collab_server" : "",
    "contents" : "# R by example: mining Twitter for consumer attitudes towards airlines from Jeffrey Breen\n# https://jeffreybreen.wordpress.com/2011/07/04/twitter-text-mining-r-slides/\n\n\n# These are the packages we need for this example - executing this line will install them\ninstall.packages(c(\"devtools\", \"rjson\", \"bit64\", \"httr\", \"plyr\", \"ggplot2\", \"doBy\", \"XML\", \"base64enc\"))\n\n# devtools allows us to install from github\nlibrary(devtools)\n\n# Here we install 2 R packages from github\ninstall_github(\"geoffjentry/twitteR\")\ninstall_github('R-package','quandl')\n\n# These are various libraries that we use throughout this example\nlibrary(plyr)\nlibrary(httr)\nlibrary(doBy)\nlibrary(Quandl)\nlibrary(twitteR)\nlibrary(ggplot2)\nlibrary(stringr)\n\n\n# To run this example you need to create a Twitter account (if you do not have one already)\n# Then create a new app at https://apps.twitter.com/\n# Go to \"Keys and Access Tokens\" and click Create my Access Token\"\n# (For more detailed instructions see: https://iag.me/socialmedia/how-to-create-a-twitter-app-in-8-easy-steps/)\n\n# Here you should input my api keys would go, to run this example you need to input your own keys, and secrets\napi_key <- \"fXLkzKTituWO1B90elOuPbTLf\"\napi_secret <- \"RUgEqGT83MLqSXFxzkko0VCMgbewKBKvUkWtaTljPi3T9Q8wdk\"\naccess_token <- \"15751972-QGwqUKyLT9qehFiTYdxunr1ySvHJl9Ph4GmvE3usI\"\naccess_token_secret <- \"sz0133LijFQih1o6JmBodl7HlYaDKAWTKH8pZ2VrCxrzh\"\n\n# Here we setup up Twitter and provide authentication details, i.e. the keys and their secrets\nsetup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)\n\n# Here we read in the two dictionaries that have been downloaded \nhu.liu.pos = scan('positive-words.txt', what='character', comment.char=';')\nhu.liu.neg = scan('negative-words.txt', what='character', comment.char=';')\n\n# Now we can add some domain-specific terminolgy\npos.words = c(hu.liu.pos, 'upgrade')\nneg.words = c(hu.liu.neg, 'wtf', 'wait', 'waiting', 'epicfail')\n\n# Our first function\nscore.sentence <- function(sentence, pos.words, neg.words) {\n  # clean up sentences with R's regex-driven global substitute, gsub():\n  sentence = gsub('[[:punct:]]', '', sentence)\n  sentence = gsub('[[:cntrl::]]', '', sentence)\n  sentence = gsub('\\\\d+', '', sentence)\n  # and convert to lower case:\n  sentence = tolower(sentence)\n  \n  #basic data structure construction\n  word.list = str_split(sentence, '\\\\s+')\n  words = unlist(word.list)\n  \n  #here we count the number of words that are positive and negative\n  pos.matches = match(words, pos.words)\n  neg.matches = match(words, neg.words)\n  \n  #throw away those that didn't match\n  pos.matches = !is.na(pos.matches)\n  neg.matches = !is.na(neg.matches)\n  \n  #compute the sentiment score\n  score = sum(pos.matches) - sum(neg.matches)\n  \n  return(score)\n}\n\n# Our second function that takes an array of sentences and sentiment analyses them\nscore.sentiment <- function(sentences, pos.words, neg.words) {\n  require(plyr)\n  require(stringr)\n  \n  #here any sentence/tweet that causes an error is given a sentiment score of 0 (neutral)\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\n    tryCatch(score.sentence(sentence, pos.words, neg.words ), error=function(e) 0)\n  }, pos.words, neg.words)\n  \n  #now we construct a data frame\n  scores.df = data.frame(score=scores, text=sentences)\n  \n  return(scores.df)\n}\n\n# Our third function, that communicates with Twitter and then scores each of the tweets returned\ncollect.and.score <- function (handle, code, airline, pos.words, neg.words) {\n  \n  tweets = searchTwitter(handle, n=1500)\n  text = laply(tweets, function(t) t$getText())\n  \n  score = score.sentiment(text, pos.words, neg.words)\n  score$airline = airline\n  score$code = code\n  \n  return (score)  \n}\n\n# Here we invoke the function above for each of our airlines\ndelta.scores = collect.and.score(\"@delta\", \"DL\", \"delta\", pos.words, neg.words)\nunited.scores = collect.and.score(\"@united\", \"UN\", \"united\", pos.words, neg.words)\njetblue.scores = collect.and.score(\"@JetBlue\", \"JB\", \"JetBlue\", pos.words, neg.words)\nus.scores = collect.and.score(\"@USAirways\", \"US\", \"USAirways\", pos.words, neg.words)\nsouthwest.scores = collect.and.score(\"@SouthwestAir\", \"SW\", \"southwest airlines\", pos.words, neg.words)\namerican.scores = collect.and.score(\"@AmericanAir\", \"AA\", \"american airlines\", pos.words, neg.words)\n\n# We can view any of these data frames using the View function, e.g.: View(delta.scores)\nView(delta.scores)\n\n# we combine all data frames into 1\nall.scores = rbind(american.scores, delta.scores, jetblue.scores, southwest.scores, united.scores, us.scores)\n\n# Skim only the most positive or negative tweets to throw away noise near 0\nall.scores$very.pos = as.numeric( all.scores$score >= 2)\nall.scores$very.neg = as.numeric( all.scores$score <= -2)\n\n# Now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline\ntwitter.df = ddply(all.scores, c('airline', 'code'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))\n\n# And here the general sentiment \ntwitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count\n\n# Now in order to be able to compare data sets we normalise the sentiment score to be a percentage\ntwitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)\n\n# And to help understand our data, order by our now normalised score\norderBy(~-score, twitter.df)\n\n\n\n# Now let's get data source no. 2, for this we need some extra packages to help us parse HTML\nlibrary(XML)\n\n# URL to American Customer Satisfaction Index\nacsi.url = 'http://www.theacsi.org/index.php?option=com_content&view=article&id=147&catid=&Itemid=212&i=Airlines'\n\n# Here we scrape out the table of interest\nacsi.df = readHTMLTable(acsi.url, header=T, which=1, stringAsFactors=F)\n\n# And access the columns we are interested in #1 (name) and #22 (2014 score)\nacsi.df = acsi.df[, c(1,22)]\n\n# Let's view our data\nhead(acsi.df, 15)\n\n# Now we add in some column names to help with the visualisation\ncolnames(acsi.df) = c('airline', 'score')\n\n# We add the airline codes we used higher up to allow us to join our two data sets\n# here NA means not applicable or in other words we are not interested in this row\n# when we join rows with NA will be thrown away\nacsi.df$code = c('JB', 'SW', NA, NA, NA, 'AA', 'DL', 'UN', NA, NA, NA, 'US', NA, NA)\n\n# Here we merge using the code we created and we also optionally apply naming standard to illustrate the source of our data\ncompare.df = merge(twitter.df, acsi.df, by='code', suffixes=c('.twitter', '.acsi'))\ncompare.df\n\nwrite.csv(compare.df, file=\"airline_results.csv\")\n\n# Plot the scores and a linear regression model\nggplot(data=compare.df) +\n    geom_point(aes(x=score.twitter, y=score.acsi, color=airline.twitter), size=5) +\n    geom_smooth(aes(x=score.twitter, y=score.acsi, group=1), se=T, method=\"lm\") +\n    theme_bw() +\n    theme(legend.position=c(0.85,0.2))\n\n\n\n# Now for data set no. 3, financial data from Quandl API\n# https://www.quandl.com/docs/api\n\n# Here we set up Quandl, putting in your authentication code\nQuandl.api_key(\"\")\n\n# Finally with one api call we can access the stock data at the time granularity of our choice\nstocks.aal = Quandl(\"YAHOO/AAL\", collapse=\"monthly\", start_date=\"2014-01-01\", type=\"ts\")\nstocks.aal\n\n\nstocks.dal = Quandl(\"YAHOO/DAL\", collapse=\"quarterly\", start_date=\"2014-01-01\", type=\"ts\")\nstocks.jaw = Quandl(\"YAHOO/F_JAW\", collapse=\"monthly\", start_date=\"2014-01-01\", type=\"ts\")\n",
    "created" : 1478337497921.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3911515948",
    "id" : "3B052C93",
    "lastKnownWriteTime" : 1478347585,
    "last_content_update" : 1478716107014,
    "path" : "~/Documents/BSHCE/Year 4/DAD/ETL Twitter Example.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}